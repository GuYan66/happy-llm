# Happy-LLM 学习笔记 Task 01: 内容介绍+前言 

> **课程来源**: [Datawhale Happy-LLM 官网](https://datawhalechina.github.io/happy-llm/)
> **我的笔记仓库**: `[https://github.com/GuYan66/happy-llm.git]`

### 课程目标

本次学习旨在系统性地理解大语言模型（LLM）的核心原理，并最终能够动手实现、训练一个属于自己的LLM。项目将理论与实践相结合，从NLP基础到动手实现LLaMA2。

### 核心收获

- **理解核心架构**：深入理解 Transformer 和注意力机制。
- **掌握训练全流程**：从预训练（Pre-training）到有监督微调（SFT）和高效微调（LoRA/QLoRA）。
- **动手搭建模型**：亲手实现一个完整的 LLaMA2 模型。
- **掌握前沿应用**：学习 RAG、Agent 等技术的实战应用。

### 学习路径

1.  **理论基础 (Ch 1-4)**:
    - NLP 基础概念
    - Transformer 架构详解
    - 预训练语言模型 (PLM)
    - 大语言模型 (LLM) 核心理论

2.  **实践应用 (Ch 5-7)**:
    - 动手从零实现 LLaMA2
    - 大模型训练实践 (预训练、微调)
    - 大模型前沿应用 (评测、RAG、Agent)

### 我的感受

课程大纲非常清晰，从理论到实践的路径设计很合理。特别是“动手搭建LLaMA2”和“掌握训练全流程”这两部分，非常吸引我。期待接下来的学习，希望能坚持完成所有任务，最终对LLM有一个全面且深入的认识。开干！
