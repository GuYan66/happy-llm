# Happy-LLM 学习笔记 Task 02: NLP 基础概念

> **课程来源**: [Datawhale Happy-LLM 官网](https://datawhalechina.github.io/happy-llm/)

> **我的笔记仓库**: [https://github.com/GuYan66/happy-llm.git](https://github.com/GuYan66/happy-llm.git)

---

## 1. NLP (自然语言处理) 的定义

NLP 是一种让计算机理解、解释和生成人类语言的技术。其核心任务是通过计算机程序来模拟人类对语言的认知和使用过程，具体的语言处理任务见后文。

## 2. NLP 发展历程

早期探索 -> 符号主义 -> 机器学习与深度学习（至今）

## 3. NLP 核心任务

-   **中文分词（Chinese Word Segmentation, CWS）**: 由于中文没有天然分隔的特点，中文分词的目的是将连续文本切分成有意义的词汇序列。
-   **子词切分 (Subword Segmentation)**: 常见的文本预处理技术，将词汇分解为更小的单位，有效处理罕见词或未见过的新词，是现代预训练模型（如BERT, GPT）的标配。
-   **词性标注（Part-of-Speech Tagging，POS Tagging）**: 为文本中的每个单词分配一个词性标签（如名词、动词、形容词等），是理解句子结构的基础。
-   **文本分类（Text Classification）**: 理解文本的含义和上下文，并基于此将文本映射到特定的类别（如情感分析、新闻分类）。
-   **实体识别（Named Entity Recognition, NER）**: 从文本中自动识别出具有特定意义的实体（如人名、地名、组织名）并进行分类。
-   **关系抽取 (Relation Extraction)**: 识别并抽取文本中实体之间的语义关系（如因果关系、拥有关系等）。
-   **文本摘要（Text Summarization）**: 生成概括原文主要内容的简洁文本，分为直接抽取式摘要（抽取关键句子或短语）和生成式摘要（生成新句子）两种方式。
-   **机器翻译（Machine Translation, MT）**: 使用计算机程序将一种自然语言（源语言）自动翻译成另一种自然语言（目标语言）。
-   **自动问答（Automatic Question Answering, QA）**: 使计算机能够理解自然语言提出的问题，并自动提供准确的答案。可以分为检索式问答（Retrieval-based QA）、知识库问答（Knowledge-based QA）和社区问答（Community-based QA）。

## 4. 文本表示的发展

在NLP中文本表示指将文本中的语言单位（如字、词、短语、句子等）以及它们之间的关系和结构信息转换为计算机能够理解和操作的形式，例如向量、矩阵或其他数据结构。发展历程如下：

### 向量空间模型 (VSM) / One-Hot
-   **原理**: 使用一个巨大的、与词表等长的向量表示一个词，词对应位置为1，其余为0。
-   **缺点**: **维度灾难**和**数据稀疏性**问题。向量维度过高；忽略了文本中的结构信息，如词序和上下文信息。

### 统计语言模型 (N-gram)
-   **核心思想**: 基于马尔可夫假设，一个词的出现概率只依赖于它前面的 N-1 个词。
-   **缺点**: 忽略了词之间的范围依赖关系，无法捕捉到句子中的复杂结构和语义信息，且当 N 较大时，会出现**数据稀疏**问题。

### 词嵌入 (Word2Vec)
-   **核心思想**: 用一个低维（几百维）的密集向量来表示词语，能有效捕捉词与词的语义关系，其核心是“上下文相似的词，其词向量也应该相似”。
-   **模型**: CBOW（根据上下文的词向量预测中心词的词向量）和 Skip-Gram（根据中心词的词向量预测上下文的词向量）。
-   **优点**: 解决了维度灾难，有助于减少计算复杂度和存储需求；并捕捉了词的语义信息，可以泛化未见过的词。
-   **缺点**: 缺乏整体的词与词之间的关系，因此在一些复杂的语义任务上表现不佳。

### 动态词向量 (ELMo)
-   **核心思想**: **一词多义**的解决方案。一个词的表示不再是固定的，而是根据其**上下文动态生成**的。
-   **实现方式**: 首先在大型语料库上训练语言模型，得到词向量模型，然后在特定任务上对模型进行微调，得到更适合该任务的词向量。使用双向LSTM结构，够捕捉到词汇的上下文信息，生成更加丰富和准确的词向量表示。

## 5. 学习总结与感受

通过对第一章的学习，我系统地回顾了NLP从诞生到如今大模型时代的完整发展脉络。最深的感触是，整个领域的发展始终围绕着“如何更有效地表示文本”这一核心问题。

从最初的One-Hot编码，到考虑上下文的N-gram，再到捕捉深层语义的Word2Vec，最后到实现“一词多义”的ELMo，每一步都是对前一步局限性的重大突破。特别是预训练思想的引入，为后续BERT等模型的爆发奠定了坚实的基础。

理解了这段演进史，也让我对当前大语言模型（LLM）所处的技术阶段和其强大能力背后的原因有了更深刻的认识。期待接下来对Transformer等核心架构的学习！
